{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import zipfile\n",
    "import collections\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import requests\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download(urls, path, filenames=None, extract=False):\n",
    "    \"\"\"Downloads a set of files.\n",
    "\n",
    "    Args:\n",
    "        urls: A (list of) urls to download files.\n",
    "        path (str): The destination path to save the files.\n",
    "        filenames: A (list of) strings of the file names. If given,\n",
    "            must have the same length with :attr:`urls`. If `None`,\n",
    "            filenames are extracted from :attr:`urls`.\n",
    "        extract (bool): Whether to extract compressed files.\n",
    "\n",
    "    Returns:\n",
    "        A list of paths to the downloaded files.\n",
    "    \"\"\"\n",
    "    maybe_create_dir(path)\n",
    "\n",
    "    if not isinstance(urls, (list, tuple)):\n",
    "        urls = [urls]\n",
    "    if filenames is not None:\n",
    "        if not isinstance(filenames, (list, tuple)):\n",
    "            filenames = [filenames]\n",
    "        if len(urls) != len(filenames):\n",
    "            raise ValueError(\n",
    "                '`filenames` must have the same number of elements as `urls`.')\n",
    "\n",
    "    result = []\n",
    "    for i, url in enumerate(urls):\n",
    "        if filenames is not None:\n",
    "            filename = filenames[i]\n",
    "        elif 'drive.google.com' in url:\n",
    "            filename = _extract_google_drive_file_id(url)\n",
    "        else:\n",
    "            filename = url.split('/')[-1]\n",
    "            # If downloading from GitHub, remove suffix ?raw=True\n",
    "            # from local filename\n",
    "            if filename.endswith(\"?raw=true\"):\n",
    "                filename = filename[:-9]\n",
    "\n",
    "        filepath = os.path.join(path, filename)\n",
    "        result.append(filepath)\n",
    "\n",
    "        if not tf.gfile.Exists(filepath):\n",
    "            if 'drive.google.com' in url:\n",
    "                filepath = _download_from_google_drive(url, filename, path)\n",
    "            else:\n",
    "                filepath = _download(url, filename, path)\n",
    "\n",
    "            if extract:\n",
    "                tf.logging.info('Extract %s', filepath)\n",
    "                if tarfile.is_tarfile(filepath):\n",
    "                    tarfile.open(filepath, 'r').extractall(path)\n",
    "                elif zipfile.is_zipfile(filepath):\n",
    "                    with zipfile.ZipFile(filepath) as zfile:\n",
    "                        zfile.extractall(path)\n",
    "                else:\n",
    "                    tf.logging.info(\"Unknown compression type. Only .tar.gz, \"\n",
    "                                    \".tar.bz2, .tar, and .zip are supported\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def _download(url, filename, path):\n",
    "    def _progress(count, block_size, total_size):\n",
    "        percent = float(count * block_size) / float(total_size) * 100.\n",
    "        # pylint: disable=cell-var-from-loop\n",
    "        sys.stdout.write('\\r>> Downloading %s %.1f%%' %\n",
    "                         (filename, percent))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    filepath = os.path.join(path, filename)\n",
    "    filepath, _ = urllib.request.urlretrieve(url, filepath, _progress)\n",
    "    print(filepath)\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded {} {} bytes.'.format(\n",
    "        filename, statinfo.st_size))\n",
    "\n",
    "    return filepath\n",
    "\n",
    "def _extract_google_drive_file_id(url):\n",
    "    # id is between `/d/` and '/'\n",
    "#     url_suffix = url[url.find('/d/')+3:]\n",
    "#     file_id = url_suffix[:url_suffix.find('/')]\n",
    "#     print(file_id)\n",
    "    file_id = url.split(\"=\")[-1]\n",
    "    return file_id\n",
    "\n",
    "def _download_from_google_drive(url, filename, path):\n",
    "    \"\"\"Adapted from `https://github.com/saurabhshri/gdrive-downloader`\n",
    "    \"\"\"\n",
    "    print(url)\n",
    "    def _get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "        return None\n",
    "\n",
    "    file_id = _extract_google_drive_file_id(url)\n",
    "\n",
    "    gurl = \"https://docs.google.com/uc?export=download\"\n",
    "    sess = requests.Session()\n",
    "    response = sess.get(gurl, params={'id': file_id}, stream=True)\n",
    "    print(response)\n",
    "    token = _get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = {'id': file_id, 'confirm': token}\n",
    "        response = sess.get(gurl, params=params, stream=True)\n",
    "\n",
    "    filepath = os.path.join(path, filename)\n",
    "    CHUNK_SIZE = 32768\n",
    "    with tf.gfile.GFile(filepath, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "    print('Successfully downloaded {}.'.format(filename))\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm /tmp/vitaFlow/conll2003.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://drive.google.com/open?id=1tdwxPJnnkyO-s1oHDETj89cfgLC2xp0c\n",
      "<Response [200]>\n",
      "Successfully downloaded 1tdwxPJnnkyO-s1oHDETj89cfgLC2xp0c.\n",
      "INFO:tensorflow:Extract /tmp/vitaFlow/1tdwxPJnnkyO-s1oHDETj89cfgLC2xp0c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/tmp/vitaFlow/1tdwxPJnnkyO-s1oHDETj89cfgLC2xp0c']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maybe_download(urls=\"https://drive.google.com/open?id=1tdwxPJnnkyO-s1oHDETj89cfgLC2xp0c\", path=\"/tmp/vitaFlow/\", filenames=\"conll2003.zip\", extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mageswarand'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.expanduser(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
